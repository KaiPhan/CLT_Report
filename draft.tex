\section*{Draft ideas}
\subsection{Introduction}
\cite{tian2022assumption}
\cite{qasim2016formalization}

\subsubsection{Motivation}
\begin{itemize}
    \item Enriching the HOL4 Theory Library
    \item Supporting Future Applications
    \item Providing Analytical Tools
\end{itemize}

\subsubsection{Contributions}
\begin{itemize}
    \item This thesis proves the Lyapunov version of the Central Limit Theorem, a fundamental result in probability theory. The CLT of Lyapunov extends the classical CLT by relaxing the identical distribution assumption to the case of independent but not necessarily identically distributed random variables.
    \item The proof makes use of Taylor expansions, moment-based bounds, and asymptotic error analysis to rigorously establish convergence to the normal distribution.
    \item The main insights involve the Lyapunov condition, which guarantees that the third absolute moments become small compared to the variance, and a new use of Big-O notation for the effective control of error terms.
\end{itemize}

\subsubsection{Results}
\begin{itemize}
    \item \textbf{Lyapunov CLT}: For independent random variables \(X_1, X_2, \ldots, X_n \) with finite means, variances, and third moments, if the Lyapunov condition is satisfied:
$$
\frac{\Gamma_n}{s_n^3} \to 0 \quad \text{as } n \to \infty,
$$
then the normalized sum \( \frac{S_n - \mu}{s_n} \) converges in distribution to the standard normal distribution \( N(0, 1) \).
    \item \textbf{Proof}: It establishes that
    \begin{itemize}
        \item Error bounds of the test function \(f\) by Taylor expansion and moment inequalities.
        \item Asymptotic vanishing of higher-order terms under the Lyapunov condition.  \item A rigorous comparison between the distribution of \( \frac{S_n}{s_n} \) and \( N(0, 1)\), bounded by \( O\left(\frac{\Gamma_n}{s_n^3}\right) \).
    \end{itemize}

These results illustrate the robustness of the Lyapunov CLT against heterogeneous distributions and, in fact, form a bridge between classical formulations and modern applications.
\end{itemize}

\subsection{Background and Related Works}
\subsubsection{Background}
Different versions of CLT \cite{fischer2011history}
\begin{itemize}
    \item \textbf{Classical Central Limit Theorem} \cite{ross2019first}
    \begin{itemize}
        \item \textbf{Proven by:} De Moivre, Laplace
        \item \textbf{Statement:} If \( X_1, X_2, \ldots, X_n \) are independent and identically distributed (i.i.d.) random variables with finite mean \( \mu \) and variance \( \sigma^2 \), then:
  \[
  Z_n = \frac{\sum_{i=1}^n X_i - n\mu}{\sigma\sqrt{n} } \xrightarrow{d} N(0, 1).
  \]
  \item \textbf{Significance:} Foundation of probability theory and statistics, used in inferential statistics, e.g., hypothesis testing and confidence intervals.
    \end{itemize}

    \item \textbf{Lyapunov’s Central Limit Theorem} \cite{chung2000course}
    \item \textbf{Lindeberg’s Central Limit Theorem}
    \item \textbf{The Generalized Central Limit Theorem} 
    \begin{itemize}
        \item https://en.wikipedia.org/wiki/Central_limit_theorem
    \end{itemize}

\end{itemize}

\subsubsection{Related Work}
The CLT is a fundamental result in probability theory, and its formalization in theorem provers has been a highly active area of interest in the formal methods community. Several different ways of proving the CLT have been tried, and tools like Isabelle/HOL and HOL4 provide different approaches reflecting their different frameworks and mathematical libraries.

\textbf{Isabelle/HOL's Formalization of the CLT}
In Isabelle/HOL, the CLT has been formalized in a powerful and especially elegant way using the \textbf{characteristic function approach}, which relies on the fact that characteristic functions uniquely determine probability distributions and their pointwise convergence implies convergence in distribution.

\begin{itemize}
    \item \textbf{Methodology}
    \item \textbf{Characteristic Function Approach}
    \begin{itemize}
        \item It provides a direct and compact route to proving the CLT, leveraging the algebraic simplicity of characteristic functions.
        \item ...
    \end{itemize}
\end{itemize}

\subsection{Premilaires}
\subsubsection{HOL4}
\cite{tian2022assumption}
\subsubsection{Measure Theory}
\cite{qasim2016formalization}
\subsubsection{Lebesgue Integration Theory}
\cite{qasim2016formalization}
\subsubsection{Probability Theory}
\cite{qasim2016formalization}

\newpage
\subsection{Central Limit Theorem}
Informal proof of Lyapounov's theorem for a single sequence by the method of Lindeberg \cite{chung2000course}.
\begin{itemize}
    \item \textbf{Notation and Statement}
    \begin{enumerate}
    \item \( \{X_j\}_{j=1}^n \) be a sequence of independent random variables with:
    \begin{itemize}
        \item[] \( \mathbb{E}[X_j] = 0 \), 
        \item[] \( \text{Var}(X_j) = \sigma_j^2 \) (finite variances, \( \sigma_j^2 < \infty \)),
        \item[] \( \mathbb{E}[|X_j|^3] = \gamma_j \) (finite third moments, \( \gamma_j < \infty \)).
    \end{itemize}

        \item Define:
        \begin{itemize}
            \item[] \( S_n = \sum_{j=1}^n X_j \),
            \item[] \( s_n^2 = \sum_{j=1}^n \sigma_j^2 \) (total variance),
            \item[] \( \Gamma_n = \sum_{j=1}^n \gamma_j \) (total third moment).
        \end{itemize}

        \item Statement:
    If 
    \[
   \frac{\Gamma_n}{s_n^3} \to 0 \quad \text{as } n \to \infty
   \]
   then 
   \[
   \frac{S_n}{s_n} \xrightarrow{d} \Phi
   \]
   where \( \Phi\) denotes the standard normal distribution \( \mathcal{N}(0,1) \).
   
    \end{enumerate}
    \item \textbf{Step-by-Step Proof}
    \begin{enumerate}
        \item \textbf{Normalize the Random Variables}
        
        Define the normalized variables:
\[
X_{n,j} = \frac{X_j}{s_n}, \quad \text{for } j = 1, \dots, n.
\]
The sum \( S_n \) is now expressed as:
\[
\frac{S_n}{s_n} = \sum_{j=1}^n X_{n,j}.
\]

\item \textbf{Use the Lindeberg Replacement to approximate}

Replace each \( X_j \) with a corresponding normal random variable \( Y_j \sim \mathcal{N}(0, \sigma_j^2) \), where \( \{Y_j\}_{j=1}^n \) are independent and have the same mean and variance as \( X_j \), then satisfy:
\[
\mathbb{E}[Y_j] = 0 \quad \text{and} \quad \text{Var}(Y_j) = \sigma_j^2.
\]

Let all the \( X\)'s and \( Y\)'s be totally independent. 

Since  \( Y_j \sim \mathcal{N}(0, \sigma_j^2) \), the normalized random variable:
\[
\frac{Y_j}{s_n} \sim \mathcal{N}\left(0, \frac{\sigma_j^2}{s_n^2}\right)
\]

Adding these normalized variables gives:
\[
\frac{1}{s_n} \sum_{j=1}^n Y_j \sim \mathcal{N}\left(0, \sum_{j=1}^n \frac{\sigma_j^2}{s_n^2}\right).
\]

Since \( s_n^2 = \sum_{j=1}^n \sigma_j^2 \), this simplifies to:
\[
\frac{1}{s_n} \sum_{j=1}^n Y_j \sim \mathcal{N}(0, 1).
\]

Now, construct the sequence \( Z\):
\[
Z_j = Y_1 + \dots + Y_{j-1} + X_j + \dots + X_n, \quad 1 \leq j \leq n
\]
Thus:
\begin{itemize}
    \item[] \( Z_1 = X_2 + X_3 + \dots + X_n \),
    \item[] \( Z_2 = Y_1 + X_3 + \dots + X_n \),
    \item[] \( Z_n = Y_1 + Y_2 + \dots + Y_{n-1} \)
\end{itemize}

Each \( Z_j \) represents a sum that is partially replaced. All variables before \( X_j \) are replaced by \( Y_j \), but the variable after \( X_j \) remains as \( X_j \).

\item \textbf{Compare Distributions}

To show that \( \frac{S_n}{s_n} \xrightarrow{d} \mathcal{N}(0, 1) \), we compare their expectations. 

\textbf{Using Test Funtions}

Consider a test function \( f \) from \( C^3_B \), the class of bounded continuous functions with three bounded derivatives.

Consider:
\[
\mathbb{E}\left[f\left(\frac{S_n}{s_n}\right)\right] \quad \text{and} \quad \mathbb{E}\left[f\left(\mathcal{N}\right)\right],
\]

By introducing the replacement sequence \( Z_j \), we rewrite:

\[
\mathbb{E}\left[f\left(\frac{S_n}{s_n}\right)\right] - \mathbb{E}\left[f\left(\frac{Z_n}{s_n}\right)\right].
\]

By telescoping:
\[
\mathbb{E}\left[f\left(\frac{S_n}{s_n}\right)\right] - \mathbb{E}\left[f\left(\frac{Z_n}{s_n}\right)\right] = \sum_{j=1}^n \left[\mathbb{E}\left[f\left(\frac{X_j + Z_j}{s_n}\right)\right] - \mathbb{E}\left[f\left(\frac{Y_j + Z_{j+1}}{s_n}\right)\right]\right]
\]

\item \textbf{Taylor Expansion of \( f \)}

By Taylor's theorem for \( f \in C^3_B \):
\[
\left|f(x + y) - \left[f(x) + f'(x)y + \frac{f''(x)}{2}y^2\right]\right| \leq \frac{M}{6}|y|^3,
\]
where \( M = \sup_{x \in \mathbb{R}} |f^{(3)}(x)| \) is the maximum bound on the third derivative of \( f \).

Apply this to \( f(\xi + \eta) \) and take Expectations, we write:
\[
\left| \mathbb{E}[f(\xi + \eta)] - \mathbb{E}[f(\xi)] - \mathbb{E}[f'(\xi)]\mathbb{E}[\eta] - \frac{1}{2}\mathbb{E}[f''(\xi)]\mathbb{E}[\eta^2] \right| \leq \frac{M}{6} \mathbb{E}[|\eta|^3],
\]
where \( \xi \) and \( \eta \) are independent random variables such that \(\mathbb{E}[|\eta|^3] < \infty \).

Then, let \( \zeta \) be another independent random variable, having the same mean and variance as \( \eta \). Now, consider:
     \[
     \mathbb{E}[f(\xi + \eta)] - \mathbb{E}[f(\xi + \zeta)].
     \]
     
Using Taylor's theorem again, this difference can be bounded as:
     \[
     \left| \mathbb{E}[f(\xi + \eta)] - \mathbb{E}[f(\xi + \zeta)] \right| \leq \frac{M}{6} (\mathbb{E}[|\eta|^3] + \mathbb{E}[|\zeta|^3]),
     \]
     where the sum \( \mathbb{E}[|\eta|^3] + \mathbb{E}[|\zeta|^3] \) accounts for the third absolute moments of both random variables.

Then, substitute with:
\begin{itemize}
    \item \( \xi = Z_j / s_n \),
    \item \( \eta = X_j / s_n \),
    \item \( \zeta = Y_j / s_n \).
\end{itemize}
We have:
   \begin{equation} \label{eq:1}
       \left| \mathbb{E}\left[f\left(\frac{Z_j}{s_n} + \frac{X_j}{s_n}\right)\right] - \mathbb{E}\left[f\left(\frac{Z_j}{s_n} + \frac{Y_j}{s_n}\right)\right] \right| \leq \frac{M}{6} \left( \mathbb{E}\left[\left| \frac{X_j}{s_n} \right|^3\right] + \mathbb{E}\left[\left| \frac{Y_j}{s_n} \right|^3\right] \right)
   \end{equation}
   
\item \textbf{Error Bound Using Lyapunov's Condition}

From the definition of \( X_j / s_n \) and \( Y_j / s_n \), we know:
     \[
     \mathbb{E}\left[\left|\frac{X_j}{s_n}\right|^3\right] = \frac{\mathbb{E}[|X_j|^3]}{s_n^3}, \quad \mathbb{E}\left[\left|\frac{Y_j}{s_n}\right|^3\right] = \frac{\mathbb{E}[|Y_j|^3]}{s_n^3}.
     \]

For \( Y_j \sim \mathcal{N}(0, \sigma_j^2) \), the third absolute moment is:
     \[
     \mathbb{E}[|Y_j|^3] = c \sigma_j^3,
     \]
     where \( c = \sqrt{8 / \pi} \) (a constant depending on the normal distribution).

Since \( \mathbb{E}[|X_j|^3] = \gamma_j \) and \(\mathbb{E}[|Y_j|^3] = c \sigma_j^3\), we wrire:
     \[
   \mathbb{E}\left[\left|\frac{X_j}{s_n}\right|^3\right] + \mathbb{E}\left[\left|\frac{Y_j}{s_n}\right|^3\right] = \frac{\gamma_j}{s_n^3} + \frac{c \sigma_j^3}{s_n^3}
   \]

By summing over \(j = 1, \dots , n\), \eqref{eq:1} becomes:
\[
\sum_{j=1}^n \left| \mathbb{E}\left[f\left(\frac{Z_j}{s_n} + \frac{X_j}{s_n}\right)\right] - \mathbb{E}\left[f\left(\frac{Z_j}{s_n} + \frac{Y_j}{s_n}\right)\right] \right| \leq \frac{M}{6} \sum_{j=1}^n \left( \frac{\gamma_j}{s_n^3} + \frac{c \sigma_j^3}{s_n^3} \right)
\]

Lyapunov's inequality states that:
\[
\sigma_j^3 \leq \gamma_j.
\]
Thus:
\[
\frac{\gamma_j}{s_n^3} + \frac{c \cdot \sigma_j^3}{s_n^3} \leq \frac{\gamma_j}{s_n^3} + \frac{c \cdot \gamma_j}{s_n^3} = \frac{(1 + c) \cdot \gamma_j}{s_n^3}.
\]
Simplify the Sum, we have:

\begin{equation} \label{eq:2}
\frac{M}{6} \sum_{j=1}^n \left( \frac{\gamma_j}{s_n^3} + \frac{c \cdot \sigma_j^3}{s_n^3} \right) \leq \frac{M}{6} \cdot \frac{1}{s_n^3} \sum_{j=1}^n (1 + c) \gamma_j
\end{equation}

Let \( K = \frac{M}{6}(1 + c) \), and the total third absolute moment of \( X_j \):
\[
\Gamma_n = \sum_{j=1}^n \gamma_j ( \space as \space define \space above)
\]
We substitute these into the inequality \eqref{eq:2}:
\begin{equation} \label{eq:3}
\frac{M}{6} \sum_{j=1}^n \left( \frac{\gamma_j}{s_n^3} + \frac{c \cdot \sigma_j^3}{s_n^3} \right) \leq K \cdot \frac{\Gamma_n}{s_n^3}
\end{equation}

By the definition of Big-O Notation, we have 
\begin{displayquote}
For a function \( f(n) \), we say that \( f(n) = O(g(n)) \) if there exist positive constants \( C > 0 \) and \( n_0 \geq 1 \) such that:
\[
|f(n)| \leq C |g(n)|, \quad \text{for all } n \geq n_0
\]
\end{displayquote}

Here:
\begin{itemize}
    \item \( f(n) = \frac{\Gamma_n}{s_n^3} \),
    \item \( g(n) = \frac{\Gamma_n}{s_n^3} \)
\end{itemize}

Clearly:
\[
|f(n)| = \left| \frac{\Gamma_n}{s_n^3} \right| \quad \text{and} \quad |g(n)| = \left| \frac{\Gamma_n}{s_n^3} \right|
\]

Since \( f(n) \) and \( g(n) \) are identical, we have:
\[
|f(n)| \leq 1 \cdot |g(n)| \quad \text{for all } n \geq 1
\]


Here, the constant \( C = 1 \). Then, the inequality \( |f(n)| \leq C |g(n)| \) holds for all \( n \geq 1 \), so we can choose \( n_0 = 1 \).

Since we have found constants \( C = 1 \) and \( n_0 = 1 \) such that:
\[
|f(n)| \leq C |g(n)| \quad \text{for all } n \geq n_0,
\]
it follows by the definition of Big-O notation that:
\begin{equation} \label{eq:4}
\frac{\Gamma_n}{s_n^3} = O\left(\frac{\Gamma_n}{s_n^3}\right)
\end{equation}

The Big O Notation property called \textbf{Multiplication by a constant} states that:
Let \( k \) be a nonzero constant. Then:
\[
O(|k| \cdot g) = O(g)
\]
Apply this to \eqref{eq:4}, we have:
   \[
   K \cdot \frac{\Gamma_n}{s_n^3} = O\left(\frac{\Gamma_n}{s_n^3}\right)
   \]
Then \eqref{eq:3} becomes:
\[
\frac{M}{6} \sum_{j=1}^n \left( \frac{\gamma_j}{s_n^3} + \frac{c \cdot \sigma_j^3}{s_n^3} \right) \leq O\left(\frac{\Gamma_n}{s_n^3}\right)
\]

We have thus obtained the following estimate:
\[
\forall f \in C^3: \left| \space  \mathbb{E}\left[f\left(\frac{S_n}{s_n}\right)\right] - \mathbb{E}\left[f\left(N\right)\right] \right| \leq O\left(\frac{\Gamma_n}{s_n^3}\right) 
\]

\item \textbf{Convergence to Normal}

By Lyapunov's condition:
\[
\frac{\Gamma_n}{s_n^3} \to 0 \quad \text{as } n \to \infty
\]
This directly implies that:
\[
\left| \mathbb{E}\left[f\left(\frac{S_n}{s_n}\right)\right] - \mathbb{E}\left[f(N)\right] \right| \to 0.
\]

Since \( \mathbb{E}[f(S_n / s_n)] \to \mathbb{E}[f(N)] \) for all bounded and continuous test functions \( f \), by the general criterion for vague convergence, we conclude that:
\[
\frac{S_n}{s_n} \xrightarrow{d} \mathcal{N}(0, 1)
\]

    \end{enumerate}
\end{itemize}




\subsection{Conclusions}
The CLT is a fundamental result in probability theory, ranging from mathematics and statistics to finance and data science. This work described the formal proof of the CLT using Lyapunov's criterion in the HOL4 theorem prover. The proof showed that the normalized sum of independent (but not necessarily identically distributed) random variables converges in distribution to the standard normal distribution by exploiting a moment-based approach along with Lyapunov's condition.

\textbf{Conclusions}
\begin{itemize}
    \item \textbf{Formalization of Lyapunov's CLT}
    \begin{itemize}
        \item The proof formalized Lyapunov's version of the CLT by providing rigorous bounds on error terms and establishing convergence under the Lyapunov condition. The use of Big-O notation and asymptotic error analysis was instrumental in managing higher-order terms.
    \end{itemize}

    \item \textbf{Theoretical Insights}
    \begin{itemize}
        \item In addition, a constructive, moment-based methodology used in its proof gave a natural perspective on finite variances and higher-order moments, each playing an important part in ensuring convergence according to CLT.
    \end{itemize}

    \item \textbf{Comparison with Previous Works}
    \begin{itemize}
        \item The approach represents a complementary alternative to existing formalizations, such as \textbf{Isabelle/HOL's characteristic function-based proof}, in that it represents a constructive proof based on explicit bounds and moment inequalities. It is also an addition to the variety of formalizations and expands the reach of theorem provers regarding probabilistic results.
    \end{itemize}

\end{itemize}
Formalization of CLT in HOL4 showcases the power of theorem provers in the formal verification of fundamental results in probability theory. It fills the gap between theoretical results and their verification by computation, showing the potential of formal methods in academic and applied domains.


\subsubsection{Future Directions}
...

In closing, this thesis provides not only a sound formalization of Lyapunov's CLT but also the whole range of insights into what happens when formal methods are integrated with probability theory. It lays the ground for further investigations into probabilistic results within theorem provers, opening perspectives to theoretical advances as well as practical applications.